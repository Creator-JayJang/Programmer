{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 class=\"site-headline\">\n",
      "<a href=\"/\"><img alt=\"python™\" class=\"python-logo\" src=\"/static/img/python-logo.png\"/></a>\n",
      "</h1>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "a = urlopen(\"http://python.org\")\n",
    "soup = BeautifulSoup(a.read(), \"html.parser\")\n",
    "print(soup.h1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Download\n",
      "End Download\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = \"http://storage.googleapis.com/patents/grant_full_text/2014/ipg140107.zip\"\n",
    "\n",
    "print(\"Start Download\")\n",
    "fname, header = urllib.request.urlretrieve(url, 'ipg140107.zip')\n",
    "print(\"End Download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "c\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"example.html\") as f:\n",
    "    soup = BeautifulSoup(f, \"html.parser\")\n",
    "    \n",
    "#a = soup.find(\"div\")\n",
    "#print(a)\n",
    "\n",
    "#b = soup.find_all(\"div\")\n",
    "#print(a)\n",
    "\n",
    "\n",
    "#a = soup.find_all(\"div\",{'id':'ABC_id'})\n",
    "#print(a)\n",
    "\n",
    "a = soup.find(\"div\",{'id':'ABC_id'}).text\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>랭킹 : 네이버 영화</title>\n",
      "1 rank : 반도\n",
      "2 rank : 강철비2: 정상회담\n",
      "3 rank : #살아있다\n",
      "4 rank : 다만 악에서 구하소서\n",
      "5 rank : 팬데믹\n",
      "6 rank : 알라딘\n",
      "7 rank : 밤쉘: 세상을 바꾼 폭탄선언\n",
      "8 rank : 소년시절의 너\n",
      "9 rank : 존 윅\n",
      "10 rank : 키싱 부스 2\n",
      "11 rank : 블루 아워\n",
      "12 rank : 프리즈너\n",
      "13 rank : 에베레스트\n",
      "14 rank : 파리의 인어\n",
      "15 rank : 강철비\n",
      "16 rank : 온워드: 단 하루의 기적\n",
      "17 rank : 1942: 언노운 배틀\n",
      "18 rank : 세인트 주디\n",
      "19 rank : 부산행\n",
      "20 rank : 올드 가드\n",
      "21 rank : 테넷\n",
      "22 rank : 마티아스와 막심\n",
      "23 rank : 비바리움\n",
      "24 rank : 키싱 부스\n",
      "25 rank : 결백\n",
      "26 rank : 빅샤크3: 젤리몬스터 대소동\n",
      "27 rank : 정도\n",
      "28 rank : 모탈: 레전드 오브 토르\n",
      "29 rank : 기기괴괴 성형수\n",
      "30 rank : 위대한 쇼맨\n",
      "31 rank : 불량한 가족\n",
      "32 rank : 슈퍼 레이스\n",
      "33 rank : 컨택트 2020\n",
      "34 rank : 모든 것을 벗어던진 특별한 여행\n",
      "35 rank : 인베이젼 2020\n",
      "36 rank : 국제수사\n",
      "37 rank : 욕창\n",
      "38 rank : 소리꾼\n",
      "39 rank : 아카이브\n",
      "40 rank : 킹스맨: 퍼스트 에이전트\n",
      "41 rank : 아디오스\n",
      "42 rank : 사라진 시간\n",
      "43 rank : 블랙아웃 : 인베이젼 어스\n",
      "44 rank : 카오산 탱고\n",
      "45 rank : 오케이 마담\n",
      "46 rank : 소년 아메드\n",
      "47 rank : 야구소녀\n",
      "48 rank : 그레이하운드\n",
      "49 rank : 우리집 똥멍청이\n",
      "50 rank : 가버나움\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "a = urlopen(\"https://movie.naver.com/movie/sdb/rank/rmovie.nhn\")\n",
    "\n",
    "soup = BeautifulSoup(a.read(), \"html.parser\")\n",
    "print(soup.title)\n",
    "\n",
    "movie = soup.find_all('div', 'tit3')\n",
    "\n",
    "i = 1\n",
    "for a in movie:\n",
    "    print(\"%d rank : %s\"%(i, a.find('a').text))\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Wordcar\n",
      "Input Number of Crawling(max 50) : 10\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "Image Crawling is done\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "baseUrl = \"https://search.naver.com/search.naver?where=image&sm=tab_jum&query=\"\n",
    "plusUrl = input(\"Input Word\")\n",
    "crawl_num = int(input(\"Input Number of Crawling(max 50) : \"))\n",
    "\n",
    "url = baseUrl + quote_plus(plusUrl)\n",
    "html = urlopen(url)\n",
    "soup = bs(html, \"html.parser\")\n",
    "img = soup.find_all(class_ = '_img')\n",
    "\n",
    "n = 1\n",
    "for i in img:\n",
    "    print(n)\n",
    "    imgUrl = i['data-source']\n",
    "    with urlopen(imgUrl) as f:\n",
    "        with open('images/img' + str(n) + '.jpg','wb') as h: # w - write b - binary\n",
    "            img = f.read()\n",
    "            h.write(img)\n",
    "        n += 1\n",
    "        if n > crawl_num:\n",
    "            break\n",
    "            \n",
    "print(\"Image Crawling is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
